from bs4 import BeautifulSoup
import selenium.webdriver as webdriver
import urllib.parse
from urllib.request import Request, urlopen
from time import sleep
import pandas as pd
import os
from urllib.request import HTTPError
from selenium.common.exceptions import NoSuchElementException


os.chdir("C:/Users/JYW/Desktop/Github/Data-Science-for-COVID-19")

url_list = {"corona":"https://www.instagram.com/explore/tags/%EC%BD%94%EB%A1%9C%EB%82%98/"
            # "oohan":"https://www.instagram.com/explore/tags/%EC%9A%B0%ED%95%9C/",
            # "dalgona":"https://www.instagram.com/explore/tags/%EB%8B%AC%EA%B3%A0%EB%82%98%EC%BB%A4%ED%94%BC/",
            # "harooJongil":"https://www.instagram.com/explore/tags/%ED%95%98%EB%A3%A8%EC%A2%85%EC%9D%BC/",
            # "zipcock":"https://www.instagram.com/explore/tags/%EC%A7%91%EC%BD%95/",
            # "SaHeojuck":"https://www.instagram.com/explore/tags/%EC%82%AC%ED%9A%8C%EC%A0%81%EA%B1%B0%EB%A6%AC%EB%91%90%EA%B8%B0/",
            # "mask":"https://www.instagram.com/explore/tags/%EB%A7%88%EC%8A%A4%ED%81%AC/",
            # "SonSSitGi":"https://www.instagram.com/explore/tags/%EC%86%90%EC%94%BB%EA%B8%B0/"
            }

for j in range(0,len(url_list)) :
    print(list(url_list.keys())[j]+"키워드 작업 중")
    url = list(url_list.values())[j]
    driver = webdriver.Chrome('C:/Users/JYW/Documents/chromedriver_win32/chromedriver.exe')

    driver.get(url)
    sleep(5)
    driver.find_element_by_xpath('//*[@id="react-root"]/section/main/article/div/ul/li[1]/button').click()

    SCROLL_PAUSE_TIME = 3.0
    reallink = []

    while True:
        pageString = driver.page_source
        bsObj = BeautifulSoup(pageString, "lxml")

        for link1 in bsObj.find_all(name="div",attrs={"class":"Nnq7C weEfm"}):
            title = link1.select('a')[0]
            real = title.attrs['href']
            reallink.append(real)
            title = link1.select('a')[1]
            real = title.attrs['href']
            reallink.append(real)
            title = link1.select('a')[2]
            real = title.attrs['href']
            reallink.append(real)

        last_height = driver.execute_script("return document.body.scrollHeight")
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        sleep(SCROLL_PAUSE_TIME)
        new_height = driver.execute_script("return document.body.scrollHeight")

        if new_height == last_height:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            sleep(SCROLL_PAUSE_TIME)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break

            else:
                last_height = new_height
                continue

    reallinknum = len(reallink)
    reallink = list(set(reallink))
    print("총"+str(reallinknum)+"개의 데이터.")

    csvtext = []
    driver = webdriver.Chrome('C:/Users/JYW/Documents/chromedriver_win32/chromedriver.exe')

    for i in range(15448,reallinknum):
        csvtext.append([])
        ### time & location
        url = 'https://www.instagram.com/p'+reallink[i]
        driver.get(url)
        sleep(1.5)

        try:
            driver.find_element_by_xpath('//*[@id="react-root"]/section/main/div/div[1]/article/div[2]/div[1]/ul/div/li/div/div/div[2]/div/div/time')
            pageString = driver.page_source
            page = BeautifulSoup(pageString, "lxml")

            loc_class = page.find_all(name="div", attrs={"class":"JF9hh"})
            time_class = page.find_all(name="div", attrs={"class":"k_Q0X NnvRN"})

            loc = str(loc_class[0].select("a"))
            loc = loc[(loc.find(">")+1):(loc.find("</"))]

            time = str(time_class[0].select("time"))
            time = time[(time.find("datetime")+10):(time.find("datetime")+20)]

            csvtext[i].append(loc)
            csvtext[i].append(time)

        except NoSuchElementException:
            print("오류발생"+str(i+1)+"번째 site")
            csvtext[i].append(0)
            csvtext[i].append(0)
            pass

        ### ID & Hashtags
        req = Request('https://www.instagram.com/p'+reallink[i],headers={'User-Agent': 'Mozilla/5.0'})
        try:
            webpage = urlopen(req).read()
        except HTTPError:
            print("오류발생"+str(i+1)+"번째 site")
            pass

        soup = BeautifulSoup(webpage,"lxml",from_encoding='utf-8')

        for reallink2 in soup.find_all("meta",attrs={"property":"instapp:hashtags"}):
            reallink2 = reallink2['content']
            csvtext[i].append(reallink2)

    data = pd.DataFrame(csvtext)

    len(data)
    data.to_csv(('insta_csvtext3.txt'), encoding='utf-8')
    data.to_csv(('insta_csvtext4.csv'), encoding='utf-8')

    data.to_csv(('insta_'+list(url_list.keys())[j]+'.txt'), encoding='utf-8')
    print(list(url_list.keys())[j]+"키워드 작업 완료")
